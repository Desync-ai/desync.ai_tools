{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: \n",
      "Number of internal links: 0\n",
      "Number of external links: 1\n",
      "Text content length: 189\n"
     ]
    }
   ],
   "source": [
    "from desync_search import DesyncClient\n",
    "\n",
    "client = DesyncClient()\n",
    "\n",
    "result = client.search(\"https://example.com\")\n",
    "\n",
    "\n",
    "### Results Display Section ####\n",
    "print(\"URL:\", result.url)\n",
    "print(\"Number of internal links:\", len(result.internal_links))\n",
    "print(\"Number of external links:\", len(result.external_links))\n",
    "print(\"Text content length:\", len(result.text_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pulling yc companies sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total links in sitemap: 5235\n"
     ]
    }
   ],
   "source": [
    "from desync_search import DesyncClient\n",
    "from desync_search import extract_links_from_sitemap\n",
    "\n",
    "client = DesyncClient()\n",
    "\n",
    "target_url = \"https://www.ycombinator.com/companies/sitemap.xml\"\n",
    "result = client.search(url=target_url, scrape_full_html=False)\n",
    "\n",
    "yc_company_profile_links = extract_links_from_sitemap(result.text_content)\n",
    "# print(links)\n",
    "print(f\"total links in sitemap: {len(yc_company_profile_links)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. pulling all yc company profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to process: 3\n",
      "Processing chunk 1/3 with 1745 links\n",
      "Retrieved 1745 pages so far.\n",
      "Processing chunk 2/3 with 1745 links\n",
      "Retrieved 3480 pages so far.\n",
      "Processing chunk 3/3 with 1745 links\n",
      "Retrieved 5225 pages so far.\n",
      "Empties: 3462\n",
      "Non-empties: 1763\n",
      "\n",
      "Reprocessing run 1/2: 3462 empty pages\n",
      "Total empty chunks to process: 2\n",
      "Reprocessing empty chunk 1/2 with 1731 links\n",
      "Reprocessing empty chunk 2/2 with 1731 links\n",
      "\n",
      "Reprocessing run 2/2: 2264 empty pages\n",
      "Total empty chunks to process: 2\n",
      "Reprocessing empty chunk 1/2 with 1132 links\n",
      "Reprocessing empty chunk 2/2 with 1132 links\n",
      "\n",
      "After reprocessing, Empties: 1139\n",
      "After reprocessing, Non-empties: 4086\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def chunk_list(lst, max_size):\n",
    "    \"\"\"\n",
    "    Splits `lst` into equal sized sublists such that none exceed `max_size` in length.\n",
    "    The sublists are as equal as possible in size.\n",
    "\n",
    "    Args:\n",
    "        lst (list): The list to be split.\n",
    "        max_size (int): The maximum size for each sublist.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sublists.\n",
    "    \"\"\"\n",
    "    n = len(lst)\n",
    "    if n <= max_size:\n",
    "        return [lst]\n",
    "    \n",
    "    # Determine how many chunks are needed.\n",
    "    number_of_chunks = math.ceil(n / float(max_size))\n",
    "    # Calculate the floor size of each chunk.\n",
    "    chunk_size_floor = n // number_of_chunks\n",
    "    # The remainder tells us how many chunks need one extra element.\n",
    "    remainder = n % number_of_chunks\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for i in range(number_of_chunks):\n",
    "        # Distribute the extra items to the first 'remainder' chunks.\n",
    "        current_chunk_size = chunk_size_floor + (1 if i < remainder else 0)\n",
    "        chunks.append(lst[start:start + current_chunk_size])\n",
    "        start += current_chunk_size\n",
    "    return chunks\n",
    "\n",
    "def reprocess_entries(yc_profile_pagedata_list):\n",
    "    reprocess_times = 2\n",
    "    for run in range(reprocess_times):\n",
    "        # Build a list of URLs from pages with empty text_content.\n",
    "        empty_links = [page.url for page in yc_profile_pagedata_list if len(page.text_content) == 0]\n",
    "        \n",
    "        # If no empty pages remain, exit early.\n",
    "        if not empty_links:\n",
    "            print(f\"No empty pages remain after {run} reprocessing runs.\")\n",
    "            break\n",
    "\n",
    "        print(f\"\\nReprocessing run {run+1}/{reprocess_times}: {len(empty_links)} empty pages\")\n",
    "        \n",
    "        # Create a mapping from URL to its index (for pages that are empty).\n",
    "        empty_indices = { page.url: idx for idx, page in enumerate(yc_profile_pagedata_list) if len(page.text_content) == 0 }\n",
    "        \n",
    "        # Split the empty URLs into chunks.\n",
    "        chunked_empty_links = chunk_list(empty_links, 2000)\n",
    "        print(f\"Total empty chunks to process: {len(chunked_empty_links)}\")\n",
    "        \n",
    "        # Process each chunk.\n",
    "        for idx, chunk in enumerate(chunked_empty_links):\n",
    "            print(f\"Reprocessing empty chunk {idx+1}/{len(chunked_empty_links)} with {len(chunk)} links\")\n",
    "            new_results = client.simple_bulk_search(target_list=chunk)\n",
    "            # Overwrite the corresponding entries in yc_profile_pagedata_list.\n",
    "            for result in new_results:\n",
    "                if result.url in empty_indices:\n",
    "                    index = empty_indices[result.url]\n",
    "                    yc_profile_pagedata_list[index] = result\n",
    "\n",
    "# --- Part 1: Initial Bulk Processing ---\n",
    "yc_profile_pagedata_list = []\n",
    "\n",
    "# Assume yc_company_profile_links is your list of profile URLs.\n",
    "chunked_links = chunk_list(yc_company_profile_links, 2000)\n",
    "print(f\"Total chunks to process: {len(chunked_links)}\")\n",
    "\n",
    "# Process each chunk and build the initial list of PageData objects.\n",
    "for idx, chunk in enumerate(chunked_links):\n",
    "    print(f\"Processing chunk {idx+1}/{len(chunked_links)} with {len(chunk)} links\")\n",
    "    results = client.simple_bulk_search(target_list=chunk)\n",
    "    yc_profile_pagedata_list.extend(results)\n",
    "    print(f\"Retrieved {len(yc_profile_pagedata_list)} pages so far.\")\n",
    "\n",
    "# Report empty vs. non-empty pages.\n",
    "empties = sum(1 for page in yc_profile_pagedata_list if len(page.text_content) == 0)\n",
    "nonempties = len(yc_profile_pagedata_list) - empties\n",
    "print(f\"Empties: {empties}\")\n",
    "print(f\"Non-empties: {nonempties}\")\n",
    "\n",
    "# --- Part 2: Reprocess Empty Pages (Multiple Runs) ---\n",
    "\n",
    "# Set the number of reprocessing attempts (default = 2).\n",
    "\n",
    "reprocess_entries(yc_profile_pagedata_list)\n",
    "\n",
    "# --- Final Reporting ---\n",
    "empties = sum(1 for page in yc_profile_pagedata_list if len(page.text_content) == 0)\n",
    "nonempties = len(yc_profile_pagedata_list) - empties\n",
    "print(f\"\\nAfter reprocessing, Empties: {empties}\")\n",
    "print(f\"After reprocessing, Non-empties: {nonempties}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. extracting profile info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yc_profile_pagedata_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Process each YC profile and build a list of dictionaries.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[1;32m     95\u001b[0m processed_profiles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[43myc_profile_pagedata_list\u001b[49m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# Use the text_content of the page to extract profile information.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     profile_info \u001b[38;5;241m=\u001b[39m extract_yc_profile_info(page\u001b[38;5;241m.\u001b[39mtext_content)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# For traceability, record the source URL.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yc_profile_pagedata_list' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_yc_profile_info(text):\n",
    "    \"\"\"\n",
    "    Given the text content of a YC company profile,\n",
    "    extracts:\n",
    "      - Company name (the line immediately preceding the first occurrence of \"Founded:\")\n",
    "      - Key/value pairs for:\n",
    "            Founded:\n",
    "            Team Size:\n",
    "            Status:\n",
    "            Location:\n",
    "            Group Partner:\n",
    "      - Tags: Lines starting 4 lines after the sequence:\n",
    "                ›\n",
    "                Companies\n",
    "                ›\n",
    "             up until the line that is exactly \"Company\".\n",
    "             Only tags that do not contain any lowercase letters are considered.\n",
    "      - URL: The first URL-like string found in the lines between \"Jobs\" and \"Founded:\".\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted data.\n",
    "    \"\"\"\n",
    "    # Split the text into non-empty, stripped lines.\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    info = {}\n",
    "    \n",
    "    # 1. Extract the key/value pairs for the known keys.\n",
    "    keys_to_extract = [\"Founded:\", \"Team Size:\", \"Status:\", \"Location:\", \"Group Partner:\"]\n",
    "    for i, line in enumerate(lines):\n",
    "        for key in keys_to_extract:\n",
    "            if line.startswith(key):\n",
    "                # Take the next line as the value (if available)\n",
    "                value = lines[i+1] if (i+1) < len(lines) else \"\"\n",
    "                # Store without the trailing colon in the key.\n",
    "                info[key.rstrip(':')] = value\n",
    "    \n",
    "    # 2. Extract the company name.\n",
    "    # We assume the company name is the line immediately preceding the first occurrence of \"Founded:\".\n",
    "    founded_index = next((i for i, line in enumerate(lines) if line.startswith(\"Founded:\")), None)\n",
    "    if founded_index is not None and founded_index > 0:\n",
    "        info[\"Company Name\"] = lines[founded_index - 1]\n",
    "    else:\n",
    "        info[\"Company Name\"] = \"Not Found\"\n",
    "    \n",
    "    # 3. Extract Tags.\n",
    "    # Look for the sequence: \"›\", \"Companies\", \"›\"\n",
    "    tags = []\n",
    "    sequence_found = False\n",
    "    for i in range(len(lines) - 2):\n",
    "        if lines[i] == \"›\" and lines[i+1] == \"Companies\" and lines[i+2] == \"›\":\n",
    "            sequence_found = True\n",
    "            # Jump 4 lines below the end of this sequence.\n",
    "            start_index = i + 2 + 3  # i+2 is the last line (\"›\"), then +3.\n",
    "            # Gather lines until we hit a line exactly equal to \"Company\".\n",
    "            j = start_index\n",
    "            while j < len(lines) and lines[j] != \"Company\":\n",
    "                tag_candidate = lines[j]\n",
    "                # Only add the tag if it does not contain any lowercase letters.\n",
    "                if tag_candidate == tag_candidate.upper():\n",
    "                    tags.append(tag_candidate)\n",
    "                j += 1\n",
    "            break  # Stop after processing the first occurrence.\n",
    "    info[\"Tags\"] = tags if sequence_found else []\n",
    "    \n",
    "    # 4. Extract URL from between \"Jobs\" and \"Founded:\".\n",
    "    # Find the index for \"Jobs\" and \"Founded:\".\n",
    "    jobs_index = None\n",
    "    # We'll consider the first occurrence of \"Jobs\" that appears before the \"Founded:\" line.\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"Jobs\" and (founded_index is None or i < founded_index):\n",
    "            jobs_index = i\n",
    "            break\n",
    "\n",
    "    found_url = None\n",
    "    if jobs_index is not None and founded_index is not None:\n",
    "        # Check the lines between \"Jobs\" (exclusive) and \"Founded:\" (exclusive) for a URL.\n",
    "        for line in lines[jobs_index+1:founded_index]:\n",
    "            # A simple regex to match an http or https URL.\n",
    "            match = re.search(r'https?://\\S+', line)\n",
    "            if match:\n",
    "                found_url = match.group(0)\n",
    "                break\n",
    "    info[\"URL\"] = found_url if found_url else \"Not Found\"\n",
    "    \n",
    "    return info\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Process each YC profile and build a list of dictionaries.\n",
    "# --------------------------------------------------\n",
    "processed_profiles = []\n",
    "\n",
    "for page in yc_profile_pagedata_list:\n",
    "    # Use the text_content of the page to extract profile information.\n",
    "    profile_info = extract_yc_profile_info(page.text_content)\n",
    "    \n",
    "    # For traceability, record the source URL.\n",
    "    profile_info[\"source_url\"] = page.url\n",
    "    \n",
    "    # Append the processed record to the list.\n",
    "    processed_profiles.append(profile_info)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Now store the processed profiles in a SQLite database.\n",
    "# The database file will be created (if it does not exist) at the specified location.\n",
    "# --------------------------------------------------\n",
    "db_path = \"/home/vlad/vlad/\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a table for storing profile information if it doesn't already exist.\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS profiles (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    company_name TEXT,\n",
    "    founded TEXT,\n",
    "    team_size TEXT,\n",
    "    status TEXT,\n",
    "    location TEXT,\n",
    "    group_partner TEXT,\n",
    "    tags TEXT,\n",
    "    url TEXT,\n",
    "    source_url TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert each processed profile into the database.\n",
    "for profile in processed_profiles:\n",
    "    company_name = profile.get(\"Company Name\", \"\")\n",
    "    founded = profile.get(\"Founded\", \"\")\n",
    "    team_size = profile.get(\"Team Size\", \"\")\n",
    "    status = profile.get(\"Status\", \"\")\n",
    "    location = profile.get(\"Location\", \"\")\n",
    "    group_partner = profile.get(\"Group Partner\", \"\")\n",
    "    # Convert the list of tags into a JSON string for storage.\n",
    "    tags = json.dumps(profile.get(\"Tags\", []))\n",
    "    url = profile.get(\"URL\", \"\")\n",
    "    source_url = profile.get(\"source_url\", \"\")\n",
    "    \n",
    "    cur.execute('''\n",
    "    INSERT INTO profiles (company_name, founded, team_size, status, location, group_partner, tags, url, source_url)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (company_name, founded, team_size, status, location, group_partner, tags, url, source_url))\n",
    "\n",
    "# Commit the changes and close the connection.\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tags Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# ----- Process and Print All Tags -----\n",
    "all_tags = []\n",
    "for profile in processed_profiles:\n",
    "    # Ensure we get a list of tags\n",
    "    tags = profile.get(\"Tags\", [])\n",
    "    if isinstance(tags, list):\n",
    "        all_tags.extend(tags)\n",
    "    else:\n",
    "        # In case tags are stored as a JSON string or other format, try to handle it\n",
    "        try:\n",
    "            import json\n",
    "            all_tags.extend(json.loads(tags))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Count the frequency of each tag.\n",
    "tag_counts = collections.Counter(all_tags)\n",
    "\n",
    "print(\"All Tags (most common to least common):\")\n",
    "for tag, count in tag_counts.most_common():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "\n",
    "# ----- Process and Print Group Partner Entries -----\n",
    "group_partners = []\n",
    "for profile in processed_profiles:\n",
    "    # Note: the extraction function stores the value under \"Group Partner\" (without the colon).\n",
    "    partner = profile.get(\"Group Partner\", \"\").strip()\n",
    "    if partner:\n",
    "        group_partners.append(partner)\n",
    "\n",
    "group_partner_counts = collections.Counter(group_partners)\n",
    "\n",
    "print(\"\\nGroup Partner (most common to least common):\")\n",
    "for partner, count in group_partner_counts.most_common():\n",
    "    print(f\"{partner}: {count}\")\n",
    "\n",
    "status_entries = []\n",
    "for profile in processed_profiles:\n",
    "    status = profile.get(\"Status\", \"\").strip()\n",
    "    if status:\n",
    "        status_entries.append(status)\n",
    "\n",
    "status_counts = collections.Counter(status_entries)\n",
    "print(\"\\nStatus (most common to least common):\")\n",
    "for status, count in status_counts.most_common():\n",
    "    print(f\"{status}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cohort Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "def is_cohort(tag):\n",
    "    \"\"\"\n",
    "    Returns True if the tag is a YC cohort tag.\n",
    "    A valid cohort tag is either:\n",
    "      - A single uppercase letter followed by two digits (e.g., \"W18\" or \"S19\"), or\n",
    "      - The special case \"IK12\".\n",
    "    \"\"\"\n",
    "    if tag == \"IK12\":\n",
    "        return True\n",
    "    if re.match(r'^[A-Z]\\d\\d$', tag):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def cohort_sort_key(cohort):\n",
    "    \"\"\"\n",
    "    Returns a tuple for sorting a cohort tag.\n",
    "    Sorting is done first by the numeric (year) part, then by a rank of the letter,\n",
    "    where the order is: W (rank 0), S (rank 1), IK (rank 2), and then others (rank 3).\n",
    "    For the special case \"IK12\", we treat it as (12, 2).\n",
    "    \"\"\"\n",
    "    if cohort == \"IK12\":\n",
    "        return (12, 2)\n",
    "    m = re.match(r'^([A-Z])(\\d\\d)$', cohort)\n",
    "    if m:\n",
    "        letter = m.group(1)\n",
    "        year = int(m.group(2))\n",
    "        letter_order_map = {\"W\": 0, \"S\": 1}  # IK is handled above.\n",
    "        letter_rank = letter_order_map.get(letter, 3)\n",
    "        return (year, letter_rank)\n",
    "    return (999, 999)  # In case of an unexpected format.\n",
    "\n",
    "# --- Build data by cohort ---\n",
    "\n",
    "# We'll build a dictionary mapping each cohort to:\n",
    "#   - \"count\": the number of companies (profiles) in that cohort\n",
    "#   - \"tag_counter\": a Counter for all non-cohort tags that appear among the companies in that cohort.\n",
    "cohort_data = defaultdict(lambda: {\"count\": 0, \"tag_counter\": Counter()})\n",
    "\n",
    "for profile in processed_profiles:\n",
    "    # Get the list of tags; if not already a list, try to decode it.\n",
    "    tags = profile.get(\"Tags\", [])\n",
    "    if not isinstance(tags, list):\n",
    "        try:\n",
    "            tags = json.loads(tags)\n",
    "        except Exception:\n",
    "            tags = []\n",
    "    \n",
    "    # Identify the cohort tags in this profile.\n",
    "    profile_cohorts = [tag for tag in tags if is_cohort(tag)]\n",
    "    if not profile_cohorts:\n",
    "        continue  # Skip profiles with no cohort tag.\n",
    "    \n",
    "    # For each profile, consider the non-cohort tags (unique per profile)\n",
    "    non_cohort_tags = set(tag for tag in tags if not is_cohort(tag))\n",
    "    \n",
    "    # Update each cohort the company belongs to.\n",
    "    for cohort in profile_cohorts:\n",
    "        cohort_data[cohort][\"count\"] += 1\n",
    "        cohort_data[cohort][\"tag_counter\"].update(non_cohort_tags)\n",
    "\n",
    "# --- Sort cohorts as required ---\n",
    "sorted_cohorts = sorted(cohort_data.keys(), key=cohort_sort_key)\n",
    "x_indices = list(range(len(sorted_cohorts)))  # For plotting along the x axis.\n",
    "cohort_labels = sorted_cohorts\n",
    "\n",
    "# --- Build a tag frequency table per cohort ---\n",
    "# We want, for each non-cohort tag, to know the fraction of companies in that cohort that hold the tag.\n",
    "all_non_cohort_tags = set()\n",
    "for data in cohort_data.values():\n",
    "    all_non_cohort_tags.update(data[\"tag_counter\"].keys())\n",
    "\n",
    "# tag_fractions will map each tag to a list of fractions (one fraction per sorted cohort).\n",
    "tag_fractions = {}\n",
    "for tag in all_non_cohort_tags:\n",
    "    fractions = []\n",
    "    for cohort in sorted_cohorts:\n",
    "        data = cohort_data[cohort]\n",
    "        total = data[\"count\"]\n",
    "        count = data[\"tag_counter\"][tag]\n",
    "        fraction = count / total if total > 0 else 0\n",
    "        fractions.append(fraction)\n",
    "    tag_fractions[tag] = fractions\n",
    "\n",
    "# --- (Optional) Select a subset of tags for clarity ---\n",
    "# For example, only include tags that ever appear in at least 5% of companies in any cohort.\n",
    "selected_tags = [tag for tag, fracs in tag_fractions.items() if max(fracs) >= 0.20]\n",
    "\n",
    "# cmap = plt.get_cmap('viridis', len(selected_tags))\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# for i, tag in enumerate(selected_tags):\n",
    "#     plt.plot(x_indices, tag_fractions[tag], marker='o', color=cmap(i), label=tag)\n",
    "\n",
    "# --- Plot the graph ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "for tag in selected_tags:\n",
    "    plt.plot(x_indices, tag_fractions[tag], marker='o', label=tag)\n",
    "\n",
    "\n",
    "plt.xticks(x_indices, cohort_labels, rotation=45)\n",
    "plt.xlabel(\"YC Cohort\")\n",
    "plt.ylabel(\"Fraction of Companies with Tag\")\n",
    "plt.title(\"Historical Frequency of Tags Across YC Cohorts\")\n",
    "plt.legend(title=\"Tag\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Clustering Analysis with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import collections\n",
    "import re\n",
    "\n",
    "# Helper function for determining if a tag is a cohort tag.\n",
    "def is_cohort(tag):\n",
    "    \"\"\"\n",
    "    Returns True if the tag is a YC cohort tag.\n",
    "    A valid cohort tag is either:\n",
    "      - A single uppercase letter followed by two digits (e.g., \"W18\" or \"S19\"), or\n",
    "      - The special case \"IK12\".\n",
    "    \"\"\"\n",
    "    if tag == \"IK12\":\n",
    "        return True\n",
    "    if re.match(r'^[A-Z]\\d\\d$', tag):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Load spaCy model (make sure you've installed a model with vectors, e.g. en_core_web_md)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# ---- Step 1: Compute Company Embeddings Based on Their Tags ----\n",
    "company_embeddings = []\n",
    "company_labels = []      # for identification (e.g., company name)\n",
    "company_tags_list = []   # non-cohort tags for each company\n",
    "\n",
    "for idx, profile in enumerate(processed_profiles):\n",
    "    tags = profile.get(\"Tags\", [])\n",
    "    # Ensure we have a list of tags\n",
    "    if not isinstance(tags, list):\n",
    "        try:\n",
    "            tags = json.loads(tags)\n",
    "        except Exception:\n",
    "            tags = []\n",
    "    # For analysis, we want the non-cohort tags\n",
    "    non_cohort_tags = [tag for tag in tags if not is_cohort(tag)]\n",
    "    \n",
    "    tag_vectors = []\n",
    "    # Use all tags (cohort or not) to compute an embedding\n",
    "    for tag in tags:\n",
    "        doc = nlp(tag)\n",
    "        if doc.has_vector:\n",
    "            tag_vectors.append(doc.vector)\n",
    "    if tag_vectors:\n",
    "        company_name = profile.get(\"Company Name\", f\"Company {idx}\")\n",
    "        # Skip profiles that don't have a valid company name.\n",
    "        if company_name == \"Not Found\":\n",
    "            continue\n",
    "        avg_vector = np.mean(tag_vectors, axis=0)\n",
    "        company_embeddings.append(avg_vector)\n",
    "        company_labels.append(company_name)\n",
    "        company_tags_list.append(non_cohort_tags)\n",
    "\n",
    "company_embeddings = np.array(company_embeddings)\n",
    "\n",
    "# ---- Step 2: Clustering Analysis (e.g., K-Means) ----\n",
    "num_clusters = 5  # adjust this as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(company_embeddings)\n",
    "\n",
    "# Compute the silhouette score to gauge clustering quality.\n",
    "score = silhouette_score(company_embeddings, clusters)\n",
    "print(\"Silhouette Score:\", score)\n",
    "\n",
    "# ---- Step 3: Dimensionality Reduction for Visualization (Optional) ----\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(company_embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
    "plt.title(\"K-Means Clustering of Companies Based on Tag Embeddings\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "plt.show()\n",
    "\n",
    "# ---- Step 4: Further Analysis by Cluster ----\n",
    "\n",
    "# (A) For each cluster, identify representative companies.\n",
    "cluster_representatives = {}\n",
    "for cluster_label in range(num_clusters):\n",
    "    # Get indices of companies in this cluster\n",
    "    indices = np.where(clusters == cluster_label)[0]\n",
    "    # Compute the Euclidean distance from each company's embedding to the cluster centroid.\n",
    "    centroid = kmeans.cluster_centers_[cluster_label]\n",
    "    distances = np.linalg.norm(company_embeddings[indices] - centroid, axis=1)\n",
    "    # Sort the indices by distance to the centroid.\n",
    "    sorted_indices = indices[np.argsort(distances)]\n",
    "    # Save the top 3 representative companies for this cluster.\n",
    "    cluster_representatives[cluster_label] = [company_labels[i] for i in sorted_indices[:3]]\n",
    "\n",
    "# (B) For each cluster, summarize the most common non-cohort tags.\n",
    "cluster_tag_summary = {}\n",
    "# We'll use a Counter for each cluster.\n",
    "for cluster_label in range(num_clusters):\n",
    "    cluster_tag_summary[cluster_label] = collections.Counter()\n",
    "\n",
    "for i, cluster_label in enumerate(clusters):\n",
    "    # Update the counter with the non-cohort tags for the company.\n",
    "    cluster_tag_summary[cluster_label].update(company_tags_list[i])\n",
    "\n",
    "# ---- Print out the analysis results ----\n",
    "for cluster_label in range(num_clusters):\n",
    "    print(f\"\\nCluster {cluster_label}:\")\n",
    "    print(\"  Representative Companies:\")\n",
    "    for comp in cluster_representatives[cluster_label]:\n",
    "        print(\"   -\", comp)\n",
    "    print(\"  Top Tags:\")\n",
    "    for tag, count in cluster_tag_summary[cluster_label].most_common(5):\n",
    "        print(f\"   - {tag}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
